{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Cross-Cloud Threat Transformer (CCTT) for Global IDS for SDN_Augmented_Datasets (CICIDS Dataset)**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport os\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, MultiHeadAttention, LayerNormalization, Dropout, Flatten, Concatenate, Reshape\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Ensure TensorFlow Uses GPU (If Available)\nprint(\"Checking GPU availability...\")\nphysical_devices = tf.config.list_physical_devices(\"GPU\")\nif physical_devices:\n    try:\n        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n        print(f\"GPU Available: {physical_devices[0]}\")\n    except RuntimeError as e:\n        print(f\"GPU Memory Growth Could Not Be Set: {e}\")\nelse:\n    print(\"No GPU found. Running on CPU.\")\n\n# ------------------------------------\n# Load & Preprocess the Dataset from Folder\n# ------------------------------------\n\n# Path to dataset folder in Kaggle\ndataset_folder = \"/kaggle/working/SDN_Augmented_Datasets\"  \n\n# List all CSV files in the folder\nfile_paths = [os.path.join(dataset_folder, file) for file in os.listdir(dataset_folder) if file.endswith(\".csv\")]\n\n# Load and combine all datasets\nall_dfs = []\nfor file in file_paths:\n    df = pd.read_csv(file)\n    df[\"Dataset\"] = file  # Add column to track dataset origin\n    all_dfs.append(df)\n\n# Merge all datasets\ncombined_df = pd.concat(all_dfs, ignore_index=True)\n\n# Standardize column names\ncombined_df.columns = combined_df.columns.str.strip().str.replace(' ', '_').str.replace('/', '_')\n\n# Print available columns\nprint(\" Available Columns:\", combined_df.columns.tolist())\n\n# Define Correct Feature List\nselected_features = [\n    \"Flow_Duration\", \"Tot_Fwd_Pkts\", \"Tot_Bwd_Pkts\", \"Fwd_Pkt_Len_Mean\",\n    \"Bwd_Pkt_Len_Mean\", \"Flow_Byts_s\", \"Flow_Pkts_s\", \"Bwd_Pkts_s\",\n    \"Bwd_Pkt_Len_Max\", \"SDN_Priority\"\n]\n\n# Check for missing columns before processing\nmissing_cols = [col for col in selected_features if col not in combined_df.columns]\nif missing_cols:\n    raise ValueError(f\"Missing Columns: {missing_cols}. Update 'selected_features' list to match dataset.\")\n\n# Encode Labels\nlabel_encoder = LabelEncoder()\ncombined_df[\"Label\"] = label_encoder.fit_transform(combined_df[\"Label\"])\n\n# Standardize numeric features\nscaler = StandardScaler()\nX = scaler.fit_transform(combined_df[selected_features])\ny = tf.keras.utils.to_categorical(combined_df[\"Label\"])  # One-hot encode labels\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Reshape inputs to be 3D for Transformer (batch_size, seq_len=1, features)\nX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\nX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n\n# ------------------------------------\n# Build the Cross-Cloud Threat Transformer (CCTT) Model\n# ------------------------------------\nclass TransformerBlock(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = tf.keras.Sequential([\n            Dense(ff_dim, activation=\"relu\"),\n            Dense(embed_dim)  # Ensure output matches input shape\n        ])\n        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n        self.dropout1 = Dropout(rate)\n        self.dropout2 = Dropout(rate)\n\n    def call(self, inputs, training=False):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n\n# Ensure input embedding size matches dataset feature size\nembed_dim = X_train.shape[-1]  # Matches input features (10)\nnum_heads = 4  # Number of attention heads\nff_dim = 32  # Feed forward network dimensions\nnum_classes = y.shape[1]  # Number of attack categories\n\n# Input Layer (with correct 3D shape)\ninputs = Input(shape=(1, X.shape[1]))\n\n#  Fix: Ensure embedding size is consistent\nx1 = Dense(embed_dim)(inputs)\nx1 = TransformerBlock(embed_dim, num_heads, ff_dim)(x1)\n\nx2 = Dense(embed_dim)(inputs)\nx2 = TransformerBlock(embed_dim, num_heads, ff_dim)(x2)\n\nx3 = Dense(embed_dim)(inputs)\nx3 = TransformerBlock(embed_dim, num_heads, ff_dim)(x3)\n\n# Cross-Segment Encoder (Merging Cloud Data)\nmerged = Concatenate()([x1, x2, x3])\nmerged = TransformerBlock(embed_dim * 3, num_heads, ff_dim)(merged)\n\n# Flatten & Fully Connected Layers\nflatten = Flatten()(merged)\ndense = Dense(64, activation=\"relu\")(flatten)\ndropout = Dropout(0.3)(dense)\noutputs = Dense(num_classes, activation=\"softmax\")(dropout)\n\n# Build Model\nCCTT_model = Model(inputs=inputs, outputs=outputs)\nCCTT_model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n# ------------------------------------\n# Train the Model (Using GPU Acceleration)\n# ------------------------------------\nprint(\"Training on GPU (if available)...\")\nhistory = CCTT_model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=25, batch_size=32)\n\n# ------------------------------------\n# Evaluate & Compare with Paperâ€™s Results\n# ------------------------------------\n# Predict on test data\ny_pred = np.argmax(CCTT_model.predict(X_test), axis=1)\ny_true = np.argmax(y_test, axis=1)\n\n# Print Accuracy & Classification Report\naccuracy = accuracy_score(y_true, y_pred)\nprint(f\"Model Accuracy: {accuracy * 100:.2f}% (Target: 98.48%)\")\nprint(classification_report(y_true, y_pred, target_names=label_encoder.classes_))\n\n# Confusion Matrix\nplt.figure(figsize=(8, 6))\ncm = confusion_matrix(y_true, y_pred)\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix - CCTT Model\")\nplt.show()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Lemurs Optimizer (LO)**# ","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# ------------------------------------\n# Define Policy Optimization Problem\n# ------------------------------------\n\n# Number of lemurs (candidate policies)\nnum_lemurs = 20\nnum_features = 2  # Two key parameters: detection threshold & computational resources\nmax_iterations = 50  # Convergence criteria\n\n# Initialize policies (lemurs) randomly\nlemurs_positions = np.random.uniform(low=[0.2, 0.5], high=[0.9, 1.5], size=(num_lemurs, num_features))\n\n# Define Fitness Function (Misclassification Rate)\ndef fitness_function(threshold, resource_allocation, model, X_test, y_test):\n    \"\"\"\n    Measures classifier performance based on misclassification rate.\n    Lower fitness = better performance.\n    \"\"\"\n    # Apply new detection threshold\n    adjusted_X_test = X_test * threshold  # Simulating adaptive thresholding\n    \n    # Adjust computational resources (simulated as batch size change)\n    batch_size = int(32 * resource_allocation)  # Dynamically adjusting batch size\n    \n    # Predict using model\n    y_pred = np.argmax(model.predict(adjusted_X_test, batch_size=batch_size), axis=1)\n    y_true = np.argmax(y_test, axis=1)\n    \n    # Compute misclassification rate\n    misclassified = np.sum(y_pred != y_true)\n    total_instances = len(y_test)\n    fitness = (misclassified / total_instances) * 100  # Formula from paper\n    \n    return fitness\n\n# Evaluate initial fitness for all lemurs\nfitness_scores = np.array([fitness_function(th, res, CCTT_model, X_test, y_test) for th, res in lemurs_positions])\n\n# Set global best position (initial best policy)\nG_best = lemurs_positions[np.argmin(fitness_scores)]  # Lemur with lowest fitness\n\n# ------------------------------------\n# Optimize Policies Using Lemurs Movement\n# ------------------------------------\n\nfor iteration in range(max_iterations):\n    for i in range(num_lemurs):\n        # Generate new position (simulate movement)\n        new_position = lemurs_positions[i] + np.random.uniform(-0.1, 0.1, size=num_features)\n\n        # Ensure position is within bounds\n        new_position = np.clip(new_position, [0.2, 0.5], [0.9, 1.5])\n\n        # Evaluate new fitness\n        new_fitness = fitness_function(new_position[0], new_position[1], CCTT_model, X_test, y_test)\n\n        # If the new position improves fitness, update position\n        if new_fitness < fitness_scores[i]:\n            lemurs_positions[i] = new_position\n            fitness_scores[i] = new_fitness\n\n        # Update Global Best\n        if new_fitness < fitness_function(G_best[0], G_best[1], CCTT_model, X_test, y_test):\n            G_best = new_position\n\n    # Print best policy found so far\n    print(f\"Iteration {iteration + 1}: Best Policy -> Threshold={G_best[0]:.4f}, Resources={G_best[1]:.4f}, Misclassification={fitness_function(G_best[0], G_best[1], CCTT_model, X_test, y_test):.2f}%\")\n\n# ------------------------------------\n# Deploy Optimal Policy to CCTT Model\n# ------------------------------------\n\noptimal_threshold, optimal_resources = G_best\nprint(f\"\\n Optimal Policy Found: Threshold={optimal_threshold:.4f}, Resources={optimal_resources:.4f}\")\n\n# Apply optimal parameters to inference\nfinal_X_test = X_test * optimal_threshold  # Adjust detection threshold\nfinal_batch_size = int(32 * optimal_resources)  # Adjust computational resources\n\n# Run final model evaluation\ny_pred_final = np.argmax(CCTT_model.predict(final_X_test, batch_size=final_batch_size), axis=1)\ny_true_final = np.argmax(y_test, axis=1)\n\n# Print final performance\naccuracy = accuracy_score(y_true_final, y_pred_final)\nprint(f\" Final Optimized Model Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}